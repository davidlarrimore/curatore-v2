# ============================================================================
# Curatore v2 - Environment Configuration
# ============================================================================
# Copy this file to .env and customize for your deployment.
# All settings have sensible defaults; customize as needed for your use case.
# ============================================================================

# ============================================================================
# API / CORS SETTINGS
# ============================================================================

# DEBUG: Enable verbose logging and development helpers
#   - true: Detailed logs, stack traces, dev tools enabled
#   - false: Production mode with minimal logging
DEBUG=false

# CORS_ORIGINS: Allowed origins for cross-origin requests (JSON array format)
#   Example: ["http://localhost:3000","http://127.0.0.1:3000"]
CORS_ORIGINS=["http://localhost:3000"]


# ============================================================================
# LLM CONFIGURATION
# ============================================================================
# The LLM is used for document quality evaluation (clarity, completeness, etc.)

# OPENAI_API_KEY: Your API key for the LLM service
#   - Required for LLM evaluation features
#   - Supports OpenAI, Ollama, OpenWebUI, LM Studio, and compatible endpoints
OPENAI_API_KEY=put-your-openai-api-key-here

# OPENAI_MODEL: Model name to use for evaluations
#   - OpenAI: gpt-4o-mini, gpt-4o, gpt-4-turbo, gpt-3.5-turbo
#   - Ollama: llama3, mistral, mixtral, etc.
#   - Recommendation: gpt-4o-mini for best cost/performance balance
OPENAI_MODEL=gpt-4o-mini

# OPENAI_BASE_URL: LLM API endpoint
#   - OpenAI: https://api.openai.com/v1
#   - Ollama: http://localhost:11434/v1
#   - OpenWebUI: http://localhost:3000/v1
#   - LM Studio: http://localhost:1234/v1
OPENAI_BASE_URL=https://api.openai.com/v1

# OPENAI_VERIFY_SSL: Verify SSL certificates for LLM API calls
#   - true: Verify certificates (recommended for production)
#   - false: Skip verification (useful for self-signed certs in dev)
OPENAI_VERIFY_SSL=true

# OPENAI_TIMEOUT: Request timeout in seconds for LLM API calls
#   - Default: 60 seconds
#   - Increase for slower models or large documents
OPENAI_TIMEOUT=60

# OPENAI_MAX_RETRIES: Number of retry attempts for failed LLM API calls
#   - Default: 3 retries
#   - Set to 0 to disable retries
OPENAI_MAX_RETRIES=3


# ============================================================================
# OCR CONFIGURATION
# ============================================================================
# Used by extraction service for image-based PDFs and scanned documents

# OCR_LANG: Tesseract OCR language code
#   - eng: English
#   - spa: Spanish
#   - fra: French
#   - deu: German
#   - Multiple languages: eng+spa+fra
#   - Full list: https://github.com/tesseract-ocr/tessdata
OCR_LANG=eng

# OCR_PSM: Tesseract Page Segmentation Mode
#   - 3: Fully automatic page segmentation (default, recommended)
#   - 6: Uniform block of text
#   - 11: Sparse text (find as much text as possible)
#   - Full list: https://tesseract-ocr.github.io/tessdoc/ImproveQuality.html#page-segmentation-method
OCR_PSM=3


# ============================================================================
# FILE UPLOAD LIMITS
# ============================================================================
# MAX_FILE_SIZE: Maximum upload size in bytes
#   - Default: 52428800 (50 MB)
#   - Formula: bytes = MB × 1024 × 1024
MAX_FILE_SIZE=52428800


# ============================================================================
# QUALITY THRESHOLDS
# ============================================================================
# All thresholds must be met for a document to be marked "RAG Ready"

# DEFAULT_CONVERSION_THRESHOLD: Conversion quality score (0-100 scale)
#   - 70+: Good quality extraction
#   - 80+: Excellent quality extraction
#   - 90+: Near-perfect extraction
DEFAULT_CONVERSION_THRESHOLD=70

# DEFAULT_CLARITY_THRESHOLD: Document clarity score (1-10 scale)
#   - How clear and understandable is the content?
#   - Evaluated by LLM
DEFAULT_CLARITY_THRESHOLD=7

# DEFAULT_COMPLETENESS_THRESHOLD: Content completeness score (1-10 scale)
#   - How complete is the extracted content?
#   - Evaluated by LLM
DEFAULT_COMPLETENESS_THRESHOLD=7

# DEFAULT_RELEVANCE_THRESHOLD: Content relevance score (1-10 scale)
#   - How relevant is the content for RAG use cases?
#   - Evaluated by LLM
DEFAULT_RELEVANCE_THRESHOLD=7

# DEFAULT_MARKDOWN_THRESHOLD: Markdown quality score (1-10 scale)
#   - How well-formatted is the markdown output?
#   - Evaluated by LLM
DEFAULT_MARKDOWN_THRESHOLD=7


# ============================================================================
# CELERY / REDIS CONFIGURATION
# ============================================================================
# Background job processing and task queue management

# CELERY_DEFAULT_QUEUE: Queue name for processing tasks
CELERY_DEFAULT_QUEUE=processing

# CELERY_BROKER_URL: Redis URL for task queue broker
#   - Format: redis://hostname:port/db
#   - Default uses service name from docker-compose
CELERY_BROKER_URL=redis://redis:6379/0

# CELERY_RESULT_BACKEND: Redis URL for storing task results
#   - Uses separate database (1) from broker (0)
CELERY_RESULT_BACKEND=redis://redis:6379/1


# ============================================================================
# JOB MANAGEMENT CONFIGURATION
# ============================================================================
# Settings for batch job processing, concurrency limits, and retention policies

# DEFAULT_JOB_CONCURRENCY_LIMIT: Maximum concurrent jobs per organization
#   - Default: 3 jobs
#   - Prevents resource exhaustion and ensures fair sharing
#   - Admins can override per organization via settings
#   - Range: 1-10 recommended
DEFAULT_JOB_CONCURRENCY_LIMIT=3

# DEFAULT_JOB_RETENTION_DAYS: Days to retain completed jobs
#   - Default: 30 days
#   - Completed jobs older than this will be auto-deleted by cleanup task
#   - Options: 7, 30, 90, or 0 for indefinite retention
#   - Admins can override per organization via settings
DEFAULT_JOB_RETENTION_DAYS=30

# JOB_CLEANUP_ENABLED: Enable automatic cleanup of expired jobs
#   - true: Run scheduled cleanup task (respects schedule below)
#   - false: Disable automatic cleanup (manual only)
#   - Default: true
#   - Cleanup deletes job records and associated files
JOB_CLEANUP_ENABLED=true

# JOB_CLEANUP_SCHEDULE_CRON: Job cleanup schedule in cron format
#   - Format: "minute hour day month day_of_week"
#   - Default: "0 3 * * *" (daily at 3 AM UTC)
#   - Examples:
#     - "0 3 * * *": Daily at 3 AM
#     - "0 */12 * * *": Every 12 hours
#     - "0 2 * * 0": Weekly on Sunday at 2 AM
JOB_CLEANUP_SCHEDULE_CRON=0 3 * * *

# JOB_CANCELLATION_TIMEOUT: Timeout in seconds for job cancellation verification
#   - Default: 30 seconds
#   - How long to wait for Celery tasks to terminate after revoke
#   - Increase if workers need more time for graceful shutdown
#   - After timeout, logs warning but continues with cleanup
JOB_CANCELLATION_TIMEOUT=30

# JOB_STATUS_POLL_INTERVAL: Polling interval in seconds for job status updates
#   - Default: 2 seconds
#   - Frontend polls at this interval for active jobs
#   - Lower = more real-time but higher load
#   - Higher = less load but slower updates
#   - Recommended: 2-5 seconds
JOB_STATUS_POLL_INTERVAL=2


# ============================================================================
# EXTRACTION SERVICE CONFIGURATION
# ============================================================================
# Settings for document-to-markdown conversion services

# EXTRACTION_SERVICE_TIMEOUT: Timeout in seconds for extraction API calls
#   - Default: 180 seconds (3 minutes)
#   - With retries: 180s → 210s → 240s (progressive timeout extension)
#   - Large PDFs or OCR-heavy documents may need longer timeouts
EXTRACTION_SERVICE_TIMEOUT=180

# DOCLING_TIMEOUT: Timeout in seconds for Docling service API calls
#   - Default: 300 seconds (5 minutes)
#   - Docling processes complex layouts and may take longer than basic extraction
DOCLING_TIMEOUT=300

# DOCLING_SERVE_MAX_SYNC_WAIT: Maximum wait time for Docling synchronous operations
#   - Only applicable when using Docling service
DOCLING_SERVE_MAX_SYNC_WAIT=300

# -----------------------------------------------------------------------------
# Apache Tika Configuration
# -----------------------------------------------------------------------------
# Settings for Apache Tika extraction engine (supports 1000+ file formats)
# Reference: https://tika.apache.org/

# TIKA_SERVICE_URL: Base URL for Apache Tika Server
#   - Default Tika port is 9998
#   - Docker: http://tika:9998
#   - Local: http://localhost:9998
# TIKA_SERVICE_URL=http://tika:9998

# TIKA_TIMEOUT: Timeout in seconds for Tika API calls
#   - Default: 300 seconds (5 minutes)
#   - Tika can handle large files; increase for very large documents
TIKA_TIMEOUT=300

# TIKA_VERIFY_SSL: Verify SSL certificates for Tika API calls
#   - true: Verify certificates (production)
#   - false: Skip verification (development with self-signed certs)
TIKA_VERIFY_SSL=true

# TIKA_ACCEPT_FORMAT: Output format preference
#   - markdown: HTML converted to Markdown (recommended for RAG)
#   - html: Raw HTML output from Tika
#   - text: Plain text output
TIKA_ACCEPT_FORMAT=markdown

# TIKA_EXTRACT_METADATA: Extract document metadata
#   - true: Include author, title, dates, etc.
#   - false: Skip metadata extraction
TIKA_EXTRACT_METADATA=true

# TIKA_OCR_LANGUAGE: OCR language for Tesseract (used by Tika for images/scanned docs)
#   - eng: English (default)
#   - spa: Spanish
#   - fra: French
#   - Multiple: eng+spa+fra
TIKA_OCR_LANGUAGE=eng


# ============================================================================
# PUBLIC SERVICE URLS (Frontend)
# ============================================================================
# Public-facing URLs for browser access to service documentation
# These map internal Docker service names to localhost URLs

# NEXT_PUBLIC_API_URL: Public URL for backend API
#   - Used by frontend to access backend services
#   - Default: http://localhost:8000 (matches docker-compose port mapping 8000:8000)
NEXT_PUBLIC_API_URL=http://localhost:8000

# NEXT_PUBLIC_EXTRACTION_URL: Public URL for extraction service
#   - Used by frontend health page to link to extraction service docs
#   - Default: http://localhost:8010 (matches docker-compose port mapping 8010:8010)
NEXT_PUBLIC_EXTRACTION_URL=http://localhost:8010

# NEXT_PUBLIC_DOCLING_URL: Public URL for Docling service
#   - Used by frontend health page to link to Docling service docs
#   - Default: http://localhost:5151 (matches docker-compose port mapping 5151:5001)
#   - Note: External port 5151 maps to internal port 5001
NEXT_PUBLIC_DOCLING_URL=http://localhost:5151

# NEXT_PUBLIC_REDIS_URL: Public URL for Redis (if exposing management UI)
#   - Default: http://localhost:6379 (matches docker-compose port mapping 6379:6379)
#   - Typically not used unless running a Redis management UI
NEXT_PUBLIC_REDIS_URL=http://localhost:6379


# ============================================================================
# LOCAL SCRIPT CONFIGURATION
# ============================================================================
# Used by SharePoint automation scripts when running on the host.

# CURATORE_API_URL: Base URL for Curatore API (defaults to NEXT_PUBLIC_API_URL)
CURATORE_API_URL=http://localhost:8000


# ============================================================================
# EXTRACTION ENGINE CONFIGURATION
# ============================================================================
# Available engines:
#   - extraction-service: Internal service using MarkItDown + Tesseract OCR
#   - docling: IBM Docling for complex PDFs and Office documents
#   - tika: Apache Tika for wide format support (1000+ types)

# ENABLE_DOCLING_SERVICE: Start Docling service container
#   - true: Start Docling container (requires more resources)
#   - false: Disable Docling (extraction-service only)
#   - Used by docker-compose profiles
ENABLE_DOCLING_SERVICE=false

# ENABLE_TIKA_SERVICE: Start Apache Tika service container
#   - true: Start Tika container
#   - false: Disable Tika (use other engines)
#   - Used by docker-compose profiles
ENABLE_TIKA_SERVICE=false

# EXTRACTION_ENGINES: Space or comma-separated list of engines to start
#   - "default": Internal extraction-service (always runs)
#   - "docling": Docling Serve (requires ENABLE_DOCLING_SERVICE=true)
#   - "tika": Apache Tika (requires ENABLE_TIKA_SERVICE=true)
#   - Example: "default docling tika" enables all
#   - Note: "default" always runs; this setting adds additional engines
EXTRACTION_ENGINES="default"

# ============================================================================
# SHAREPOINT / MICROSOFT GRAPH CONFIGURATION
# ============================================================================
# Used by scripts that connect to SharePoint via Microsoft Graph.

# MS_TENANT_ID: Azure AD tenant ID (GUID)
MS_TENANT_ID=

# MS_CLIENT_ID: Azure AD app registration client ID
MS_CLIENT_ID=

# MS_CLIENT_SECRET: Azure AD app registration client secret
MS_CLIENT_SECRET=

# MS_GRAPH_SCOPE: OAuth scope for app-only access
#   - Default: https://graph.microsoft.com/.default
MS_GRAPH_SCOPE=https://graph.microsoft.com/.default

# MS_GRAPH_BASE_URL: Microsoft Graph API base URL
#   - Default: https://graph.microsoft.com/v1.0
MS_GRAPH_BASE_URL=https://graph.microsoft.com/v1.0


# ============================================================================
# DATABASE CONFIGURATION
# ============================================================================
# Multi-tenant persistence layer for organizations, users, connections, and settings

# DATABASE_URL: SQLAlchemy database connection URL
#   SQLite (development):
#     - sqlite+aiosqlite:///./data/curatore.db
#     - File-based, zero configuration, good for single-node deployments
#   PostgreSQL (production):
#     - postgresql+asyncpg://user:password@localhost:5432/curatore
#     - Recommended for production, multi-worker support, better concurrency
#   Default: SQLite
DATABASE_URL=sqlite+aiosqlite:///./data/curatore.db

# DB_POOL_SIZE: Database connection pool size (PostgreSQL only)
#   - Default: 20 connections
#   - Increase for high-traffic deployments
DB_POOL_SIZE=20

# DB_MAX_OVERFLOW: Maximum overflow connections (PostgreSQL only)
#   - Default: 40 additional connections beyond pool_size
#   - Total max connections = pool_size + max_overflow
DB_MAX_OVERFLOW=40

# DB_POOL_RECYCLE: Connection recycle time in seconds (PostgreSQL only)
#   - Default: 3600 (1 hour)
#   - Connections are recycled after this time to prevent stale connections
DB_POOL_RECYCLE=3600


# ============================================================================
# AUTHENTICATION & SECURITY
# ============================================================================
# JWT token authentication and API key management

# JWT_SECRET_KEY: Secret key for signing JWT tokens
#   - CRITICAL: Change this in production!
#   - Generate with: openssl rand -hex 32
#   - Keep this secret and never commit to version control
JWT_SECRET_KEY=your-secret-key-change-in-production

# JWT_ALGORITHM: Algorithm for JWT token signing
#   - Default: HS256 (HMAC with SHA-256)
#   - Other options: HS384, HS512, RS256 (requires RSA keys)
JWT_ALGORITHM=HS256

# JWT_ACCESS_TOKEN_EXPIRE_MINUTES: Access token expiration in minutes
#   - Default: 60 minutes (1 hour)
#   - Shorter = more secure but more frequent refreshes
#   - Longer = better UX but wider attack window
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=60

# JWT_REFRESH_TOKEN_EXPIRE_DAYS: Refresh token expiration in days
#   - Default: 30 days
#   - Used to obtain new access tokens without re-login
JWT_REFRESH_TOKEN_EXPIRE_DAYS=30

# BCRYPT_ROUNDS: Bcrypt hashing work factor
#   - Default: 12 rounds
#   - Higher = more secure but slower
#   - Lower = faster but less secure
#   - Recommended: 12-14 for production
BCRYPT_ROUNDS=12

# API_KEY_PREFIX: Prefix for API keys
#   - Default: "cur_"
#   - Used for display purposes (e.g., cur_1234abcd...)
API_KEY_PREFIX=cur_


# ============================================================================
# EMAIL CONFIGURATION
# ============================================================================
# Email delivery for user notifications (verification, password reset, etc.)

# EMAIL_BACKEND: Email backend to use
#   Options:
#     - console: Log emails to console (development/testing)
#     - smtp: Send via SMTP server (production)
#     - sendgrid: Send via SendGrid API (requires sendgrid Python package)
#     - ses: Send via AWS Simple Email Service (requires boto3 Python package)
#   Default: console
EMAIL_BACKEND=console

# EMAIL_FROM_ADDRESS: From email address for outgoing emails
#   - Used as sender address
#   - Should be a verified domain in production
EMAIL_FROM_ADDRESS=noreply@curatore.app

# EMAIL_FROM_NAME: From name for outgoing emails
#   - Displayed as sender name
EMAIL_FROM_NAME=Curatore

# FRONTEND_BASE_URL: Frontend base URL for email links
#   - Used to generate verification and password reset links
#   - Should match your frontend deployment URL
FRONTEND_BASE_URL=http://localhost:3000

# -----------------------------------------------------------------------------
# SMTP Configuration (if EMAIL_BACKEND=smtp)
# -----------------------------------------------------------------------------

# SMTP_HOST: SMTP server hostname
#   Examples:
#     - Gmail: smtp.gmail.com
#     - Outlook: smtp.office365.com
#     - SendGrid: smtp.sendgrid.net
SMTP_HOST=

# SMTP_PORT: SMTP server port
#   - 587: TLS/STARTTLS (recommended)
#   - 465: SSL
#   - 25: Unencrypted (not recommended)
SMTP_PORT=587

# SMTP_USERNAME: SMTP authentication username
#   - Usually your email address
SMTP_USERNAME=

# SMTP_PASSWORD: SMTP authentication password
#   - For Gmail: Use app-specific password, not your regular password
#   - Keep this secret and never commit to version control
SMTP_PASSWORD=

# SMTP_USE_TLS: Use TLS encryption
#   - true: Use TLS (recommended for port 587)
#   - false: No TLS (only use for port 465 with SSL)
SMTP_USE_TLS=true

# -----------------------------------------------------------------------------
# SendGrid Configuration (if EMAIL_BACKEND=sendgrid)
# -----------------------------------------------------------------------------

# SENDGRID_API_KEY: SendGrid API key
#   - Get from: https://app.sendgrid.com/settings/api_keys
#   - Keep this secret and never commit to version control
SENDGRID_API_KEY=

# -----------------------------------------------------------------------------
# AWS SES Configuration (if EMAIL_BACKEND=ses)
# -----------------------------------------------------------------------------

# AWS_REGION: AWS region for SES
#   - Example: us-east-1, us-west-2, eu-west-1
AWS_REGION=us-east-1

# AWS_ACCESS_KEY_ID: AWS access key ID
#   - Optional: If not provided, uses IAM role or default credentials
AWS_ACCESS_KEY_ID=

# AWS_SECRET_ACCESS_KEY: AWS secret access key
#   - Optional: If not provided, uses IAM role or default credentials
#   - Keep this secret and never commit to version control
AWS_SECRET_ACCESS_KEY=

# -----------------------------------------------------------------------------
# Token Expiration Settings
# -----------------------------------------------------------------------------

# EMAIL_VERIFICATION_TOKEN_EXPIRE_HOURS: Email verification token expiration
#   - Default: 24 hours
#   - How long verification links remain valid
EMAIL_VERIFICATION_TOKEN_EXPIRE_HOURS=24

# PASSWORD_RESET_TOKEN_EXPIRE_HOURS: Password reset token expiration
#   - Default: 1 hour
#   - How long password reset links remain valid
#   - Keep short for security
PASSWORD_RESET_TOKEN_EXPIRE_HOURS=1

# EMAIL_VERIFICATION_GRACE_PERIOD_DAYS: Grace period before enforcing verification
#   - Default: 7 days
#   - Users can access the app for this many days without verifying email
#   - After grace period, email verification is required
EMAIL_VERIFICATION_GRACE_PERIOD_DAYS=7


# ============================================================================
# MULTI-TENANCY & ORGANIZATIONS
# ============================================================================
# Organization-based multi-tenancy settings

# ENABLE_AUTH: Enable authentication layer
#   - true: Require JWT or API key authentication for all endpoints
#   - false: Allow unauthenticated access (backward compatibility mode)
#   - Default: false (for backward compatibility with ENV-based config)
#   - Set to true after running seed command and creating initial admin user
ENABLE_AUTH=false

# DEFAULT_ORG_ID: Default organization ID for unauthenticated requests
#   - Only used when ENABLE_AUTH=false
#   - Set to UUID of default organization (created by seed command)
#   - Leave empty to use first organization in database
DEFAULT_ORG_ID=

# AUTO_TEST_CONNECTIONS: Automatically test connections on save
#   - true: Run health check when creating/updating connections
#   - false: Skip automatic testing (test manually via API)
#   - Default: true
#   - Provides immediate feedback on connection validity
AUTO_TEST_CONNECTIONS=true


# ============================================================================
# INITIAL SEEDING (First-time Setup)
# ============================================================================
# Used by seed command to create initial organization and admin user
# Run: python -m app.commands.seed --create-admin

# ADMIN_EMAIL: Initial admin user email
#   - Used for first login
#   - Change after first login recommended
ADMIN_EMAIL=admin@example.com

# ADMIN_USERNAME: Initial admin username
#   - Used for first login
ADMIN_USERNAME=admin

# ADMIN_PASSWORD: Initial admin password
#   - CRITICAL: Change immediately after first login!
#   - Never use default password in production
ADMIN_PASSWORD=changeme

# ADMIN_FULL_NAME: Initial admin full name
ADMIN_FULL_NAME=Admin User

# DEFAULT_ORG_NAME: Default organization name
#   - Created during initial seed
DEFAULT_ORG_NAME=Default Organization

# DEFAULT_ORG_SLUG: Default organization URL slug
#   - Used in URLs and identifiers
#   - Must be URL-safe (lowercase, no spaces)
DEFAULT_ORG_SLUG=default


# ============================================================================
# OBJECT STORAGE CONFIGURATION (MinIO / S3) - REQUIRED
# ============================================================================
# Curatore v2 requires object storage. Filesystem storage is no longer supported.
# Backend connects directly to MinIO or S3 for all file operations.
#
# Start services with: ./scripts/dev-up.sh
# Initialize storage with: ./scripts/init_storage.sh
#
# IMPORTANT: For development, you need to:
#   1. MinIO starts automatically with backend services (no --profile needed)
#   2. Run ./scripts/init_storage.sh to create buckets and set lifecycle policies
#
#   NOTE: As of v2.3+, all file operations are proxied through the backend API.
#   You NO LONGER need to:
#     - Add "127.0.0.1 minio" to /etc/hosts
#     - Configure MINIO_PUBLIC_ENDPOINT or MINIO_PRESIGNED_ENDPOINT
#   The backend handles all MinIO communication internally, and the browser
#   downloads files via authenticated backend endpoints.

# USE_OBJECT_STORAGE: Enable object storage (REQUIRED)
#   - true: Store files in object storage (REQUIRED - no filesystem fallback)
#   - false: DEPRECATED - filesystem storage is no longer supported
USE_OBJECT_STORAGE=true

# -----------------------------------------------------------------------------
# MinIO/S3 Connection Settings
# -----------------------------------------------------------------------------

# MINIO_ENDPOINT: MinIO/S3 server endpoint (host:port) for backend connections
#   - This is the internal endpoint the backend uses to connect to MinIO
#   - MinIO (Docker): minio:9000 (default)
#   - MinIO (Local): localhost:9000
#   - AWS S3: s3.amazonaws.com
#   - AWS S3 (regional): s3.us-west-2.amazonaws.com
MINIO_ENDPOINT=minio:9000

# DEPRECATED: The following settings are no longer needed with proxy architecture
# All file operations are proxied through the backend, eliminating the need for
# direct browser-to-MinIO communication and environment-specific endpoint configuration.

# MINIO_PRESIGNED_ENDPOINT: DEPRECATED - No longer needed
#   Legacy: Endpoint used to generate presigned URLs
# MINIO_PRESIGNED_ENDPOINT=

# MINIO_PUBLIC_ENDPOINT: DEPRECATED - No longer needed
#   Legacy: Public endpoint for presigned URLs (client access)
# MINIO_PUBLIC_ENDPOINT=

# MINIO_ACCESS_KEY: MinIO access key / AWS Access Key ID
#   - CRITICAL: Change default in production!
#   - Should match MINIO_ROOT_USER for MinIO
MINIO_ACCESS_KEY=minioadmin

# MINIO_SECRET_KEY: MinIO secret key / AWS Secret Access Key
#   - CRITICAL: Change default in production!
#   - Keep secret, never commit to version control
#   - Should match MINIO_ROOT_PASSWORD for MinIO
MINIO_SECRET_KEY=minioadmin

# MINIO_SECURE: Use HTTPS for internal backend-to-MinIO connections
#   - false: HTTP (development, internal networks, MinIO default)
#   - true: HTTPS (production, AWS S3, public networks)
MINIO_SECURE=false

# MINIO_PUBLIC_SECURE: DEPRECATED - No longer needed with proxy architecture
#   Legacy: Use HTTPS for presigned URLs
#   - false: HTTP (development)
#   - true: HTTPS (production)
# MINIO_PUBLIC_SECURE=false

# -----------------------------------------------------------------------------
# Bucket Configuration
# -----------------------------------------------------------------------------

# MINIO_BUCKET_UPLOADS: Bucket for uploaded files
#   - Stores original uploaded documents
#   - Default retention: 7 days (via bucket lifecycle)
MINIO_BUCKET_UPLOADS=curatore-uploads

# MINIO_BUCKET_PROCESSED: Bucket for processed files
#   - Stores processed markdown output
#   - Default retention: 30 days (via bucket lifecycle)
MINIO_BUCKET_PROCESSED=curatore-processed

# MINIO_BUCKET_TEMP: Bucket for temporary files
#   - Stores temporary processing files
#   - Default retention: 1 day (via bucket lifecycle)
MINIO_BUCKET_TEMP=curatore-temp

# Bucket Display Names (shown in UI)
# MINIO_BUCKET_UPLOADS_DISPLAY_NAME: Friendly name for uploads bucket
#   - Default: "Default Storage"
#   - Used in storage browser and job creation UI
MINIO_BUCKET_UPLOADS_DISPLAY_NAME=Default Storage

# MINIO_BUCKET_PROCESSED_DISPLAY_NAME: Friendly name for processed files bucket
#   - Default: "Processed Files"
#   - Used in storage browser and job creation UI
MINIO_BUCKET_PROCESSED_DISPLAY_NAME=Processed Files

# MINIO_BUCKET_TEMP_DISPLAY_NAME: Friendly name for temporary files bucket
#   - Default: "Temporary Files"
#   - Used in storage browser and job creation UI
MINIO_BUCKET_TEMP_DISPLAY_NAME=Temporary Files

# MINIO_PRESIGNED_EXPIRY: Presigned URL expiry in seconds
#   - Default: 3600 (1 hour)
#   - Frontend uploads/downloads use presigned URLs for direct storage access
#   - Lower = more secure, higher = more convenient
MINIO_PRESIGNED_EXPIRY=3600

# FILE_RETENTION_UPLOADED_DAYS: Retention period for uploaded files (days)
#   - Default: 30
#   - Files expire after this many days in object storage
#   - Can be managed by S3 lifecycle policies
FILE_RETENTION_UPLOADED_DAYS=30

# FILE_RETENTION_PROCESSED_DAYS: Retention period for processed files (days)
#   - Default: 90
#   - Processed markdown files expire after this many days
FILE_RETENTION_PROCESSED_DAYS=90

# FILE_RETENTION_TEMP_DAYS: Retention period for temporary files (days)
#   - Default: 7
#   - Temporary processing files expire after this many days
FILE_RETENTION_TEMP_DAYS=7

# -----------------------------------------------------------------------------
# MinIO Docker Container Configuration
# -----------------------------------------------------------------------------
# These settings configure the MinIO Docker container itself.
# Only needed when using MinIO as the storage provider (not AWS S3).

# MINIO_ROOT_USER: MinIO admin username (for Docker container)
#   - Used to configure MinIO container on startup
#   - Should match MINIO_ACCESS_KEY for consistency
#   - Default: minioadmin
MINIO_ROOT_USER=minioadmin

# MINIO_ROOT_PASSWORD: MinIO admin password (for Docker container)
#   - Used to configure MinIO container on startup
#   - Should match MINIO_SECRET_KEY for consistency
#   - CRITICAL: Change in production!
MINIO_ROOT_PASSWORD=minioadmin

# =============================================================================
# OPENSEARCH CONFIGURATION (Native Full-Text Search)
# =============================================================================
# OpenSearch provides full-text search across all indexed content (uploads,
# SharePoint, web scrapes). Documents are automatically indexed after extraction.
#
# Quick Start:
#   1. Enable OpenSearch: OPENSEARCH_ENABLED=true
#   2. Start with search profile: docker compose --profile search up -d
#   3. Access search at: http://localhost:3000/search
#
# Architecture:
#   - OpenSearch runs as a separate container (port 9200)
#   - Assets are indexed after successful extraction
#   - Indices are organization-scoped for multi-tenancy
#   - Full-text search with relevance scoring and highlighting
# -----------------------------------------------------------------------------

# OPENSEARCH_ENABLED: Enable native full-text search
#   - false: Search disabled (default, smaller footprint)
#   - true: Enable OpenSearch search functionality
OPENSEARCH_ENABLED=false

# OPENSEARCH_ENDPOINT: OpenSearch server endpoint (host:port)
#   - Docker default: opensearch:9200
#   - Local: localhost:9200
#   - AWS OpenSearch: your-domain.region.es.amazonaws.com
OPENSEARCH_ENDPOINT=opensearch:9200

# OPENSEARCH_USERNAME: OpenSearch username (optional)
#   - Leave empty for no authentication (dev mode with security disabled)
#   - Set for production deployments with security enabled
OPENSEARCH_USERNAME=

# OPENSEARCH_PASSWORD: OpenSearch password (optional)
#   - Leave empty for no authentication (dev mode with security disabled)
#   - Set for production deployments with security enabled
OPENSEARCH_PASSWORD=

# OPENSEARCH_VERIFY_SSL: Verify SSL certificates
#   - false: Disable SSL verification (development, self-signed certs)
#   - true: Verify SSL (production, AWS OpenSearch)
OPENSEARCH_VERIFY_SSL=false

# OPENSEARCH_INDEX_PREFIX: Prefix for all Curatore indices
#   - Default: curatore
#   - Indices are named: {prefix}-assets-{org_id}
OPENSEARCH_INDEX_PREFIX=curatore

# OPENSEARCH_BATCH_SIZE: Batch size for bulk indexing operations
#   - Default: 100
#   - Used during reindex operations
OPENSEARCH_BATCH_SIZE=100

# OPENSEARCH_SEARCH_TIMEOUT: Timeout for search requests (seconds)
#   - Default: 30
#   - Increase for large indices or complex queries
OPENSEARCH_SEARCH_TIMEOUT=30

# OPENSEARCH_MAX_CONTENT_LENGTH: Maximum content length to index (characters)
#   - Default: 100000
#   - Longer documents are truncated for indexing
OPENSEARCH_MAX_CONTENT_LENGTH=100000


# =============================================================================
# SAM.GOV CONFIGURATION (Federal Opportunities API)
# =============================================================================
# SAM.gov Opportunities API integration for federal contract data.
# Get your API key at: https://api.sam.gov
#
# Features:
#   - Search for opportunities by NAICS, PSC, agency, keywords
#   - Track solicitations and amendments over time
#   - Download and extract attachments
#   - Generate LLM-powered summaries
#
# -----------------------------------------------------------------------------

# SAM_API_KEY: Your SAM.gov API key (required for SAM.gov integration)
#   - Get your API key at: https://api.sam.gov
#   - Free tier allows 1000 requests per day
SAM_API_KEY=

# SAM_ENABLED: Enable SAM.gov integration
#   - true: Enable SAM.gov features
#   - false: Disable SAM.gov features (default)
SAM_ENABLED=false

# Note: SAM.gov API base URL is hardcoded to https://api.sam.gov/opportunities/v2
# and cannot be configured (fixed government endpoint)

# SAM_TIMEOUT: Request timeout in seconds
#   - Default: 60
SAM_TIMEOUT=60

# SAM_RATE_LIMIT_DELAY: Delay between API requests in seconds
#   - Default: 0.5
#   - Used for rate limiting to avoid hitting API limits
SAM_RATE_LIMIT_DELAY=0.5

# SAM_QUEUE_PROCESS_ENABLED: Enable automatic processing of queued requests
#   - Default: true
#   - When rate limits are exceeded, requests are queued and processed later
SAM_QUEUE_PROCESS_ENABLED=true

# SAM_QUEUE_PROCESS_INTERVAL: Interval in seconds for checking queued requests
#   - Default: 300 (5 minutes)
#   - How often the worker checks for queued requests to process
SAM_QUEUE_PROCESS_INTERVAL=300
