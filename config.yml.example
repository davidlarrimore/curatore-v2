# ============================================================================
# Curatore v2 - Service Configuration
# ============================================================================
# Copy this file to config.yml and customize for your environment.
#
# Environment Variable References:
#   Use ${VAR_NAME} to reference environment variables from .env
#   Example: api_key: ${OPENAI_API_KEY}
#
# Optional Settings:
#   Settings marked with "(optional)" have sensible defaults
#   and can be omitted if defaults are acceptable.
#
# Configuration Priority:
#   1. config.yml (if present)
#   2. Environment variables from .env
#   3. Built-in defaults
# ============================================================================

version: "2.0"

# ============================================================================
# LLM Configuration (optional)
# ============================================================================
# Language model configuration for AI-powered features.
#
# Supports: OpenAI, Ollama, OpenWebUI, LM Studio, and compatible endpoints
# If not configured here, falls back to OPENAI_* environment variables
#
# The 'models' section allows configuring different models for different tasks:
#   - embedding: For semantic search (generates vector embeddings)
#   - summarization: For SAM.gov and document summaries
#   - evaluation: For document quality assessment
#   - general: For other LLM tasks (document improvement, etc.)

llm:
  # Provider name (openai, ollama, openwebui, lmstudio)
  provider: openai

  # API key or authentication token
  # Recommended: Reference from environment for security
  api_key: ${OPENAI_API_KEY}

  # API endpoint URL
  # Examples:
  #   - OpenAI: https://api.openai.com/v1
  #   - Ollama: http://localhost:11434/v1
  #   - OpenWebUI: http://localhost:3000/v1
  #   - LM Studio: http://localhost:1234/v1
  base_url: https://api.openai.com/v1

  # Default model identifier (used when task-specific model not configured)
  # Examples:
  #   - OpenAI: gpt-4o-mini, gpt-4o, gpt-4-turbo
  #   - Ollama: llama3, mistral, mixtral
  model: gpt-4o-mini

  # Request timeout in seconds (optional, default: 60)
  # Increase for slower models or large documents
  timeout: 60

  # Maximum retry attempts (optional, default: 3)
  max_retries: 3

  # Default generation temperature (optional, default: 0.7)
  # Range: 0.0 (deterministic) to 2.0 (creative)
  temperature: 0.7

  # Verify SSL certificates (optional, default: true)
  # Set to false for self-signed certificates in development
  verify_ssl: true

  # Provider-specific options (optional)
  # Pass additional parameters to the LLM provider
  options:
    max_tokens: 4096
    top_p: 1.0

  # ---------------------------------------------------------------------------
  # Task-Specific Models (optional)
  # ---------------------------------------------------------------------------
  # Configure which model to use for different tasks. All models use the
  # connection settings above (api_key, base_url, provider).
  #
  # If not configured, falls back to the default 'model' above.
  models:
    # Embedding model for semantic search
    # Options: text-embedding-3-small (recommended), text-embedding-3-large
    embedding:
      model: text-embedding-3-small

    # Summarization model for SAM.gov and document summaries
    summarization:
      model: gpt-4o-mini
      temperature: 0.3  # Lower for consistent summaries

    # Evaluation model for document quality scoring
    evaluation:
      model: gpt-4o-mini
      temperature: 0.3  # Lower for consistent scoring

    # General model for other tasks (improvement, optimization)
    general:
      model: gpt-4o-mini


# ============================================================================
# Extraction Engine Configuration (optional)
# ============================================================================
# Document-to-markdown conversion engines
#
# Curatore supports multiple extraction engines for document conversion.
# Enable/disable engines as needed for your use case.
#
# Available engines:
#   - document-service: External service using MarkItDown + Tesseract OCR (curatore-document-service)
#   - docling: IBM Docling for complex PDFs and Office documents
#   - tika: Apache Tika for wide format support (1000+ types)
#
# If not configured here, falls back to DOCUMENT_SERVICE_* environment variables

extraction:
  # Default engine to use when none specified (optional, default: document-service)
  # IMPORTANT: This must match the 'name' field of one of the enabled engines below
  # Options: document-service, docling, tika
  default_engine: document-service

  # Intelligent Triage-Based Extraction Routing
  # Documents are automatically analyzed upfront to select the optimal extraction engine:
  # - fast_pdf: PyMuPDF for simple text-based PDFs (fastest)
  # - fast_office: Native Python libs for simple Office files (fast)
  # - docling: Advanced extraction for complex documents (slower but high-quality)
  # - ocr_only: OCR extraction for images and scanned documents
  #
  # Benefits:
  # - Routes simple documents to fast local engines
  # - Routes complex documents directly to Docling
  # - Sub-second triage analysis

  # List of available extraction engines
  # Users can select these when creating jobs via Connections UI
  engines:
    # -------------------------------------------------------------------------
    # Document Service (Recommended)
    # -------------------------------------------------------------------------
    # External service using MarkItDown + Tesseract OCR (curatore-document-service)
    # Best for: General documents, PDFs, Office files, images with text
    # API Endpoint: /api/v1/extract (automatically used based on engine_type)
    - name: document-service
      display_name: "Document Service"
      description: "External document service using MarkItDown and Tesseract OCR"

      # Engine type identifier (required)
      # This determines which extraction logic and API endpoint to use
      # API endpoints are predefined per engine_type and cannot be overridden
      engine_type: document-service

      # Service base URL (required)
      service_url: http://document-service:8010

      # Request timeout in seconds (optional, default: 300)
      timeout: 300

      # Enable this engine (required)
      # Set to false to disable without removing configuration
      enabled: true

      # Verify SSL certificates (optional, default: true)
      verify_ssl: true

      # Docling OCR toggle (optional, default: true)
      # When set, this maps to Docling's enable_ocr option.
      docling_ocr_enabled: true

      # OCR settings (Tesseract-specific, optional)
      # These settings apply to the document-service's OCR processing
      ocr:
        # Tesseract OCR language code (optional, default: eng)
        # Examples: eng, spa, fra, deu, or combinations like "eng+spa"
        # See: https://github.com/tesseract-ocr/tesseract/blob/main/doc/tesseract.1.asc#languages
        language: eng

        # Page Segmentation Mode (optional, default: 3)
        # 3 = Fully automatic page segmentation (recommended)
        # 6 = Uniform block of text
        # 11 = Sparse text (find as much text as possible)
        # See: https://github.com/tesseract-ocr/tesseract/blob/main/doc/tesseract.1.asc#options
        psm: 3

    # -------------------------------------------------------------------------
    # Docling (IBM Research)
    # -------------------------------------------------------------------------
    # Advanced extraction engine for complex PDFs and Office documents
    # Best for: Academic papers, technical documents, complex layouts
    # Requires: Docling service running (see docker-compose.yml)
    # API Endpoint: /v1/convert/file (automatically used based on engine_type)
    - name: docling
      display_name: "Docling (IBM Research)"
      description: "Advanced extraction for complex PDFs and Office documents"

      # Engine type identifier (required)
      # This determines which extraction logic and API endpoint to use
      # API endpoints are predefined per engine_type and cannot be overridden
      engine_type: docling

      # Docling service URL (required)
      # Note: Docling typically runs on port 5001
      service_url: http://docling:5001

      # Request timeout in seconds (optional, default: 600)
      # Docling can be slower for complex documents
      timeout: 600

      # Enable this engine (required)
      # Set to true to enable Docling extraction
      # Requires: ENABLE_DOCLING_SERVICE=true in docker-compose
      enabled: false

      # Verify SSL certificates (optional, default: true)
      verify_ssl: true

      # Docling-specific options (optional)
      # These are passed as query parameters to Docling's /v1/convert/file endpoint
      # See: https://deepwiki.com/docling-project/docling-serve/4.1-conversion-endpoints
      options:
        # Output format (required, default: markdown)
        output_format: markdown

        # Image export mode (optional, default: placeholder)
        # Options: placeholder, embedded, none
        image_export_mode: placeholder

        # Pipeline type (optional, default: standard)
        # Options: standard, fast, accurate
        pipeline_type: standard

        # Enable OCR for scanned documents (optional, default: true)
        enable_ocr: true

        # OCR engine (optional, default: auto)
        # Options: auto, tesseract, easyocr
        ocr_engine: auto

        # Table extraction mode (optional, default: accurate)
        # Options: accurate, fast, none
        table_mode: accurate

    # -------------------------------------------------------------------------
    # Apache Tika
    # -------------------------------------------------------------------------
    # Popular open-source content analysis toolkit from Apache Software Foundation
    # Best for: Wide format support (1000+ types), metadata extraction, archives
    # Requires: Tika Server running (see docker-compose.yml)
    # API Endpoint: /tika (automatically used based on engine_type)
    # Reference: https://cwiki.apache.org/confluence/display/TIKA/TikaServer
    - name: tika
      display_name: "Apache Tika"
      description: "Wide format support (1000+ types) with metadata extraction"

      # Engine type identifier (required)
      # This determines which extraction logic and API endpoint to use
      engine_type: tika

      # Tika Server URL (required)
      # Default port is 9998
      service_url: http://tika:9998

      # Request timeout in seconds (optional, default: 300)
      timeout: 300

      # Enable this engine (required)
      # Set to true to enable Tika extraction
      enabled: false

      # Verify SSL certificates (optional, default: true)
      verify_ssl: true

      # Tika-specific options (optional)
      options:
        # Output format preference (optional, default: markdown)
        # Options:
        #   - markdown: HTML converted to Markdown (recommended for RAG)
        #   - html: Raw HTML output
        #   - text: Plain text output
        accept_format: markdown

        # Extract document metadata (optional, default: true)
        # Extracts author, title, creation date, etc.
        extract_metadata: true

        # OCR strategy for images/scanned PDFs (optional)
        # Options:
        #   - no_ocr: Skip OCR entirely
        #   - ocr_only: Only use OCR (ignore embedded text)
        #   - ocr_and_text: Use both OCR and embedded text
        # ocr_strategy: ocr_and_text

        # OCR language for Tesseract (optional, default: eng)
        # Examples: eng, spa, fra, deu, or combinations like "eng+spa"
        ocr_language: eng

        # PDF OCR strategy (optional)
        # Controls how PDFs with images are processed
        # pdf_strategy: ocr_and_text


# ============================================================================
# Playwright Rendering Service Configuration (optional)
# ============================================================================
# Browser-based rendering service for JavaScript-heavy web scraping.
# Required for crawling Single Page Applications (SPAs) and dynamic content.
#
# The Playwright service renders pages with a real browser, executes JavaScript,
# and extracts content inline during crawl (no separate extraction job needed).
#
# If not configured here, falls back to PLAYWRIGHT_* environment variables

playwright:
  # Enable Playwright rendering service (optional, default: true if URL provided)
  enabled: true

  # Playwright service URL (required to enable)
  # Examples:
  #   - Docker: http://playwright:8011
  #   - Local: http://localhost:8011
  service_url: http://playwright:8011

  # Request timeout in seconds (optional, default: 60)
  # Total time allowed for page render + content extraction
  timeout: 60

  # Maximum retry attempts for failed requests (optional, default: 3)
  max_retries: 3

  # Browser pool size (optional, default: 3)
  # Number of browser instances to maintain
  browser_pool_size: 3

  # Default viewport dimensions (optional)
  default_viewport_width: 1920
  default_viewport_height: 1080

  # Default page load timeout in ms (optional, default: 60000)
  # Increase this for slow-loading sites (max: 300000 = 5 minutes)
  default_timeout_ms: 60000

  # Default wait for selector timeout in ms (optional, default: 5000)
  default_wait_timeout_ms: 5000

  # File extensions to identify as downloadable documents (optional)
  # When crawling web pages, links to these file types will be extracted
  document_extensions:
    - ".pdf"
    - ".docx"
    - ".doc"
    - ".xlsx"
    - ".xls"
    - ".pptx"
    - ".ppt"


# ============================================================================
# Microsoft Graph API Configuration (optional)
# ============================================================================
# Microsoft Graph API integration for SharePoint, OneDrive, and other M365 services
#
# Requires:
#   - Azure AD app registration with client credentials
#   - Microsoft Graph API permissions: Sites.Read.All or Files.Read.All
#   - Admin consent granted for the application
#
# If not configured here, falls back to MS_* environment variables

microsoft_graph:
  # Enable Microsoft Graph integration (optional, default: true)
  enabled: true

  # Azure AD tenant ID (GUID format)
  # Recommended: Reference from environment for security
  tenant_id: ${MS_TENANT_ID}

  # Azure AD app registration client ID
  client_id: ${MS_CLIENT_ID}

  # Azure AD app registration client secret
  # CRITICAL: Keep this secret! Reference from environment.
  client_secret: ${MS_CLIENT_SECRET}

  # OAuth scope (optional, default: https://graph.microsoft.com/.default)
  graph_scope: https://graph.microsoft.com/.default

  # Microsoft Graph API base URL (optional)
  graph_base_url: https://graph.microsoft.com/v1.0

  # Request timeout in seconds (optional, default: 60)
  timeout: 60

  # Maximum retry attempts (optional, default: 3)
  max_retries: 3

  # Enable email sending via Microsoft Graph API (optional, default: false)
  # When enabled, emails are sent via Graph API instead of SMTP/SendGrid/SES.
  # Requires Mail.Send application permission in Azure AD.
  enable_email: false

  # Email sender user ID or UPN (required when enable_email is true)
  # This must be a valid mailbox in your Microsoft 365 tenant.
  # Example: noreply@yourdomain.com
  email_sender_user_id: ${MS_EMAIL_SENDER_USER_ID}


# ============================================================================
# Email Service Configuration (optional)
# ============================================================================
# Email delivery for user notifications (verification, password reset, etc.)
#
# Supported backends:
#   - console: Log emails to console (development/testing)
#   - smtp: Send via SMTP server (production)
#   - sendgrid: Send via SendGrid API
#   - ses: Send via AWS Simple Email Service
#
# If not configured here, falls back to EMAIL_* environment variables

email:
  # Email backend (console, smtp, sendgrid, ses)
  backend: console  # Change to 'smtp' for production

  # From email address
  # Should be a verified domain in production
  from_address: noreply@curatore.app

  # From display name
  from_name: Curatore

  # -----------------------------------------------------------------------------
  # SMTP Configuration (required if backend=smtp)
  # -----------------------------------------------------------------------------
  smtp:
    # SMTP server hostname
    # Examples:
    #   - Gmail: smtp.gmail.com
    #   - Outlook: smtp.office365.com
    #   - SendGrid: smtp.sendgrid.net
    host: ${SMTP_HOST}

    # SMTP server port (optional, default: 587)
    # - 587: TLS/STARTTLS (recommended)
    # - 465: SSL
    # - 25: Unencrypted (not recommended)
    port: 587

    # SMTP authentication username (optional)
    # Usually your email address
    username: ${SMTP_USERNAME}

    # SMTP authentication password (optional)
    # For Gmail: Use app-specific password, not your regular password
    # CRITICAL: Keep this secret! Reference from environment.
    password: ${SMTP_PASSWORD}

    # Use TLS encryption (optional, default: true)
    use_tls: true

    # Connection timeout in seconds (optional, default: 30)
    timeout: 30

  # -----------------------------------------------------------------------------
  # SendGrid Configuration (required if backend=sendgrid)
  # -----------------------------------------------------------------------------
  # sendgrid:
  #   api_key: ${SENDGRID_API_KEY}

  # -----------------------------------------------------------------------------
  # AWS SES Configuration (required if backend=ses)
  # -----------------------------------------------------------------------------
  # aws_ses:
  #   region: us-east-1
  #   access_key_id: ${AWS_ACCESS_KEY_ID}  # Optional, uses IAM role if not provided
  #   secret_access_key: ${AWS_SECRET_ACCESS_KEY}  # Optional


# ============================================================================
# Queue Configuration (optional)
# ============================================================================
# Celery background job processing settings

queue:
  # Redis URL for task queue broker (optional, default: redis://redis:6379/0)
  broker_url: redis://redis:6379/0

  # Redis URL for storing task results (optional, default: redis://redis:6379/1)
  result_backend: redis://redis:6379/1

  # Default queue name (optional, default: processing)
  default_queue: processing

  # Worker concurrency level (optional, default: 4)
  # Number of worker processes per container
  worker_concurrency: 4

  # Task timeout in seconds (optional, default: 3600)
  # Maximum time a task can run before being terminated
  task_timeout: 3600


# ============================================================================
# Queue Type Settings (optional)
# ============================================================================
# Per-queue-type configuration for background job processing.
# Queue types are defined in backend/app/services/queue_registry.py.
# This section provides RUNTIME PARAMETER OVERRIDES for those queue types.
#
# Available queue types:
#   - extraction: Document-to-Markdown conversion with triage-based engine selection
#   - sam: SAM.gov federal opportunity data pulls
#   - scrape: Web scraping and crawling
#   - sharepoint: SharePoint document synchronization
#   - maintenance: System maintenance and cleanup tasks
#
# Configurable parameters per queue:
#   - max_concurrent: Maximum jobs running simultaneously (null = unlimited)
#   - timeout_seconds: Job timeout in seconds
#   - submission_interval: Seconds between queue processing checks
#   - duplicate_cooldown: Seconds before allowing duplicate job
#   - enabled: Enable/disable the queue type

queues:
  # Extraction queue - triage-based document conversion
  # Uses intelligent routing to select optimal engine:
  #   - fast_pdf: Simple text-based PDFs (PyMuPDF)
  #   - document-service: Office files, text, emails (MarkItDown)
  #   - docling: Complex PDFs, large Office files (OCR + layout)
  extraction:
    # Maximum concurrent extractions (default: 10)
    # Increase for faster throughput, decrease to reduce resource usage
    max_concurrent: 10

    # Timeout for extraction jobs in seconds (default: 600 = 10 minutes)
    timeout_seconds: 600

  # SAM.gov queue - federal opportunity data pulls
  # sam:
  #   max_concurrent: null  # Unlimited by default
  #   timeout_seconds: 1800

  # Web scraping queue
  # scrape:
  #   max_concurrent: null  # Unlimited by default
  #   timeout_seconds: 3600

  # SharePoint sync queue
  # sharepoint:
  #   max_concurrent: null  # Unlimited by default
  #   timeout_seconds: 1800


# ============================================================================
# MinIO / S3 Object Storage Configuration (REQUIRED)
# ============================================================================
# Curatore v2 requires object storage. Filesystem storage is no longer supported.
# Backend connects directly to MinIO or S3 for all file operations.
#
# Start services with: ./scripts/dev-up.sh
# Initialize storage with: ./scripts/init_storage.sh
#
# IMPORTANT: For development, you need to:
#   1. MinIO starts automatically with backend services (no --profile needed)
#   2. Add "127.0.0.1 minio" to your /etc/hosts (or C:\Windows\System32\drivers\etc\hosts)
#      This allows your browser to access presigned URLs that use the "minio" hostname.
#   3. Run ./scripts/init_storage.sh to create buckets and set lifecycle policies
#   For production, configure public_endpoint to match your public S3/MinIO hostname.
#
# If not configured here, falls back to MINIO_* environment variables

minio:
  # Enable object storage (REQUIRED, default: true)
  # Object storage is required - no filesystem fallback
  enabled: true

  # MinIO/S3 server endpoint (host:port) for internal operations (required)
  # Examples:
  #   - MinIO (Docker): minio:9000
  #   - MinIO (Local): localhost:9000
  #   - AWS S3: s3.amazonaws.com
  #   - AWS S3 (regional): s3.us-west-2.amazonaws.com
  endpoint: minio:9000

  # Endpoint used to generate presigned URLs (optional)
  # Must be reachable from the backend container
  # Leave empty to use 'endpoint' value
  # presigned_endpoint:

  # Public endpoint for presigned URLs (optional)
  # The hostname that appears in presigned URLs for browser/client access
  # Leave empty for development (requires /etc/hosts entry: 127.0.0.1 minio)
  # Production: your-bucket.s3.amazonaws.com or custom domain
  # public_endpoint:

  # MinIO access key / AWS Access Key ID (required)
  # Recommended: Reference from environment for security
  # Should match MINIO_ROOT_USER for MinIO
  access_key: ${MINIO_ACCESS_KEY}

  # MinIO secret key / AWS Secret Access Key (required)
  # CRITICAL: Keep this secret! Reference from environment.
  # Should match MINIO_ROOT_PASSWORD for MinIO
  secret_key: ${MINIO_SECRET_KEY}

  # Use HTTPS for internal MinIO/S3 connections (optional, default: false)
  # - false: HTTP (development, internal networks, MinIO default)
  # - true: HTTPS (production, AWS S3, public networks)
  secure: false

  # Use HTTPS for presigned URLs (optional, defaults to 'secure' if not set)
  # - false: HTTP (development)
  # - true: HTTPS (production)
  # public_secure: false

  # Bucket for uploaded files (optional, default: curatore-uploads)
  bucket_uploads: curatore-uploads

  # Bucket for processed files (optional, default: curatore-processed)
  bucket_processed: curatore-processed

  # Bucket for temporary files (optional, default: curatore-temp)
  bucket_temp: curatore-temp

  # Presigned URL expiry in seconds (optional, default: 3600 = 1 hour)
  # Range: 60 seconds (1 min) to 86400 seconds (24 hours)
  # Frontend uploads/downloads use presigned URLs for direct storage access
  # Lower = more secure, higher = more convenient
  presigned_expiry: 3600


# ============================================================================
# Search Configuration (PostgreSQL + pgvector Hybrid Search)
# ============================================================================
# Curatore uses PostgreSQL with pgvector for hybrid full-text + semantic search.
# Documents are automatically chunked, embedded, and indexed after extraction.
#
# Features:
#   - Full-text search: PostgreSQL tsvector + GIN indexes (keyword matching)
#   - Semantic search: pgvector embeddings (finds related content)
#   - Hybrid search: Combines both for best results (default mode)
#   - No external services required (uses same PostgreSQL database)
#
# Embedding Model:
#   Configure the embedding model in llm.models.embedding above.
#   Default: text-embedding-3-small (OpenAI, 1536 dimensions)
#
# Search Modes:
#   - keyword: Full-text search only (fast, exact matches)
#   - semantic: Vector similarity only (finds related content)
#   - hybrid: Combines both with configurable weighting (recommended)

search:
  # Enable search functionality (optional, default: true)
  enabled: true

  # Default search mode (optional, default: hybrid)
  # Options: keyword, semantic, hybrid
  default_mode: hybrid

  # Semantic weight in hybrid search (optional, default: 0.5)
  # Range: 0.0 (keyword only) to 1.0 (semantic only)
  # 0.5 gives equal weight to both
  semantic_weight: 0.5

  # Request timeout in seconds (optional, default: 30)
  timeout: 30

  # Batch size for bulk indexing operations (optional, default: 50)
  batch_size: 50

  # Maximum content length to index (optional, default: 100000 characters)
  max_content_length: 100000

  # Chunk size for document splitting (optional, default: 1500)
  chunk_size: 1500

  # Overlap between chunks (optional, default: 200)
  chunk_overlap: 200


# ============================================================================
# SAM.gov API Configuration (Federal Opportunities)
# ============================================================================
# SAM.gov Opportunities API integration for federal contract data ingestion
# and analysis. Enables searching, tracking, and summarizing federal
# solicitations.
#
# Get an API key at: https://api.sam.gov
#
# Features:
#   - Search for opportunities by NAICS, PSC, agency, keywords
#   - Track solicitations and amendments over time
#   - Download and extract attachments (SOWs, pricing templates, etc.)
#   - Generate LLM-powered summaries and compliance checklists
#
# If not configured here, falls back to SAM_* environment variables

sam:
  # Enable SAM.gov integration (optional, default: true)
  enabled: true

  # SAM.gov API key (required)
  # Get your API key at: https://api.sam.gov
  # Recommended: Reference from environment for security
  api_key: ${SAM_API_KEY}

  # Note: API base URL is hardcoded to https://api.sam.gov/opportunities/v2
  # and cannot be configured (SAM.gov has a fixed API endpoint)

  # Request timeout in seconds (optional, default: 60)
  timeout: 60

  # Maximum retry attempts (optional, default: 3)
  max_retries: 3

  # Delay between API requests in seconds (optional, default: 0.5)
  # Used for rate limiting to avoid hitting API limits
  rate_limit_delay: 0.5

  # Maximum pages to fetch per pull operation (optional, default: 10)
  # Each page contains up to page_size results
  max_pages_per_pull: 10

  # Number of results per page (optional, default: 100)
  # SAM.gov API supports up to 1000 per request
  page_size: 100
