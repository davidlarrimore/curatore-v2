# ============================================================================
# Curatore v2 - Docker Compose (dev)
# ============================================================================
version: "3.9"

services:
  # --------------------------------------------------------------------------
  # BACKEND: FastAPI app
  # --------------------------------------------------------------------------
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: curatore-backend
    volumes:
      # Hot-reload app code without stomping other layers
      - ./backend/app:/app/app
      # Hot-reload alembic migrations
      - ./backend/alembic:/app/alembic
      # Application data directory (logs, temp files)
      - ./backend/data:/app/data
      # YAML configuration (optional, falls back to env vars if not present)
      - ./config.yml:/app/config.yml:ro
    environment:
      - PYTHONUNBUFFERED=1
      - DEBUG=${DEBUG:-false}
      - CORS_ORIGINS=${CORS_ORIGINS:-["http://localhost:3000"]}
      # Database (PostgreSQL)
      - DATABASE_URL=${DATABASE_URL:-postgresql+asyncpg://curatore:curatore_dev_password@postgres:5432/curatore}
      - DB_POOL_SIZE=${DB_POOL_SIZE:-20}
      - DB_MAX_OVERFLOW=${DB_MAX_OVERFLOW:-40}
      - DB_POOL_RECYCLE=${DB_POOL_RECYCLE:-3600}
      # LLM
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4o-mini}
      - OPENAI_BASE_URL=${OPENAI_BASE_URL:-https://api.openai.com/v1}
      - OPENAI_VERIFY_SSL=${OPENAI_VERIFY_SSL:-true}
      - OPENAI_TIMEOUT=${OPENAI_TIMEOUT:-60}
      - OPENAI_MAX_RETRIES=${OPENAI_MAX_RETRIES:-3}
      # OCR
      - OCR_LANG=${OCR_LANG:-eng}
      - OCR_PSM=${OCR_PSM:-3}
      # FILE UPLOAD LIMITS
      - MAX_FILE_SIZE=${MAX_FILE_SIZE:-52428800}
      # Extraction service (optional; when enabled backend will call it for PDFs/DOCX, etc.)
      - EXTRACTION_SERVICE_URL=http://extraction:8010
      - EXTRACTION_SERVICE_TIMEOUT=${EXTRACTION_SERVICE_TIMEOUT:-180}
      - EXTRACTION_SERVICE_VERIFY_SSL=true
      # Docling configuration
      # NOTE: DOCLING_SERVICE_URL is now managed in config.yml (extraction engines)
      # Uncomment below only if using legacy env var approach instead of config.yml
      # - DOCLING_SERVICE_URL=http://docling:5001
      # Docling extract path is fixed per API (/v1/extract)
      - DOCLING_TIMEOUT=${DOCLING_TIMEOUT:-300}
      - DOCLING_VERIFY_SSL=true
      # Playwright rendering service (for JavaScript-rendered web scraping)
      - PLAYWRIGHT_SERVICE_URL=http://playwright:8011
      - PLAYWRIGHT_TIMEOUT=${PLAYWRIGHT_TIMEOUT:-60}
      # SharePoint / Microsoft Graph
      - MS_TENANT_ID=${MS_TENANT_ID:-}
      - MS_CLIENT_ID=${MS_CLIENT_ID:-}
      - MS_CLIENT_SECRET=${MS_CLIENT_SECRET:-}
      - MS_GRAPH_SCOPE=${MS_GRAPH_SCOPE:-https://graph.microsoft.com/.default}
      - MS_GRAPH_BASE_URL=${MS_GRAPH_BASE_URL:-https://graph.microsoft.com/v1.0}
      # Object Storage (direct MinIO connection - REQUIRED)
      - USE_OBJECT_STORAGE=${USE_OBJECT_STORAGE:-true}
      - MINIO_ENDPOINT=minio:9000  # Override .env for inter-container communication
      - MINIO_PUBLIC_ENDPOINT=${MINIO_PUBLIC_ENDPOINT:-}
      - MINIO_PRESIGNED_ENDPOINT=${MINIO_PRESIGNED_ENDPOINT:-}
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY:-${MINIO_ROOT_USER:-admin}}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY:-${MINIO_ROOT_PASSWORD:-changeme}}
      - MINIO_SECURE=${MINIO_SECURE:-false}
      - MINIO_PUBLIC_SECURE=${MINIO_PUBLIC_SECURE:-false}
      - MINIO_BUCKET_UPLOADS=${MINIO_BUCKET_UPLOADS:-curatore-uploads}
      - MINIO_BUCKET_PROCESSED=${MINIO_BUCKET_PROCESSED:-curatore-processed}
      - MINIO_BUCKET_TEMP=${MINIO_BUCKET_TEMP:-curatore-temp}
      - MINIO_PRESIGNED_EXPIRY=${MINIO_PRESIGNED_EXPIRY:-3600}
      # Search (PostgreSQL + pgvector)
      - SEARCH_ENABLED=${SEARCH_ENABLED:-true}
      # Celery (for queueing indexing tasks)
      - CELERY_BROKER_URL=${CELERY_BROKER_URL:-redis://redis:6379/0}
      - CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND:-redis://redis:6379/1}
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    ports:
      - "8000:8000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    depends_on:
      - redis
      - minio

  # --------------------------------------------------------------------------
  # WORKER: Celery worker
  # --------------------------------------------------------------------------
  worker:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: curatore-worker
    # Auto-restart worker on code changes using watchdog/watchmedo
    # Queue order: priority first, then by job type, maintenance last
    # Auto-restart worker on code changes using watchdog/watchmedo
    # Queue order: priority first, then by job type, maintenance last
    command: >
      sh -c "watchmedo auto-restart --directory=/app/app --pattern=*.py --recursive --
      celery -A app.celery_app worker -Q processing_priority,extraction,sam,scrape,sharepoint,maintenance -l info --concurrency=${CELERY_CONCURRENCY:-4}"
    environment:
      - PYTHONUNBUFFERED=1
      # Mark as Celery worker for database service (uses NullPool to avoid event loop issues)
      - CELERY_WORKER=1
      # Database (PostgreSQL)
      - DATABASE_URL=${DATABASE_URL:-postgresql+asyncpg://curatore:curatore_dev_password@postgres:5432/curatore}
      - DB_POOL_SIZE=${DB_POOL_SIZE:-20}
      - DB_MAX_OVERFLOW=${DB_MAX_OVERFLOW:-40}
      - DB_POOL_RECYCLE=${DB_POOL_RECYCLE:-3600}
      - CELERY_BROKER_URL=${CELERY_BROKER_URL:-redis://redis:6379/0}
      - CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND:-redis://redis:6379/1}
      - CELERY_DEFAULT_QUEUE=${CELERY_DEFAULT_QUEUE:-processing}
      # Worker concurrency - should be >= sum of all queue max_concurrent in config.yml
      - CELERY_CONCURRENCY=${CELERY_CONCURRENCY:-4}
      # Extraction service (match backend)
      - EXTRACTION_SERVICE_URL=http://extraction:8010
      - EXTRACTION_SERVICE_TIMEOUT=${EXTRACTION_SERVICE_TIMEOUT:-180}
      - EXTRACTION_SERVICE_VERIFY_SSL=true
      # Docling (match backend)
      # NOTE: DOCLING_SERVICE_URL is now managed in config.yml (extraction engines)
      # Uncomment below only if using legacy env var approach instead of config.yml
      # - DOCLING_SERVICE_URL=http://docling:5001
      # Docling extract path is fixed per API (/v1/extract)
      - DOCLING_TIMEOUT=${DOCLING_TIMEOUT:-300}
      - DOCLING_VERIFY_SSL=true
      # Playwright rendering service (match backend)
      - PLAYWRIGHT_SERVICE_URL=http://playwright:8011
      - PLAYWRIGHT_TIMEOUT=${PLAYWRIGHT_TIMEOUT:-60}
      # SharePoint / Microsoft Graph
      - MS_TENANT_ID=${MS_TENANT_ID:-}
      - MS_CLIENT_ID=${MS_CLIENT_ID:-}
      - MS_CLIENT_SECRET=${MS_CLIENT_SECRET:-}
      - MS_GRAPH_SCOPE=${MS_GRAPH_SCOPE:-https://graph.microsoft.com/.default}
      - MS_GRAPH_BASE_URL=${MS_GRAPH_BASE_URL:-https://graph.microsoft.com/v1.0}
      # Object Storage (direct MinIO connection - REQUIRED)
      - USE_OBJECT_STORAGE=${USE_OBJECT_STORAGE:-true}
      - MINIO_ENDPOINT=minio:9000  # Override .env for inter-container communication
      - MINIO_PUBLIC_ENDPOINT=${MINIO_PUBLIC_ENDPOINT:-}
      - MINIO_PRESIGNED_ENDPOINT=${MINIO_PRESIGNED_ENDPOINT:-}
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY:-${MINIO_ROOT_USER:-admin}}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY:-${MINIO_ROOT_PASSWORD:-changeme}}
      - MINIO_SECURE=${MINIO_SECURE:-false}
      - MINIO_PUBLIC_SECURE=${MINIO_PUBLIC_SECURE:-false}
      - MINIO_BUCKET_UPLOADS=${MINIO_BUCKET_UPLOADS:-curatore-uploads}
      - MINIO_BUCKET_PROCESSED=${MINIO_BUCKET_PROCESSED:-curatore-processed}
      - MINIO_BUCKET_TEMP=${MINIO_BUCKET_TEMP:-curatore-temp}
      - MINIO_PRESIGNED_EXPIRY=${MINIO_PRESIGNED_EXPIRY:-3600}
      # Search (PostgreSQL + pgvector)
      - SEARCH_ENABLED=${SEARCH_ENABLED:-true}
    volumes:
      - ./backend/app:/app/app
      # Application data directory (shared with backend)
      - ./backend/data:/app/data
      # YAML configuration (optional, falls back to env vars if not present)
      - ./config.yml:/app/config.yml:ro
    depends_on:
      - redis
      - minio

  # --------------------------------------------------------------------------
  # BEAT: Celery Beat scheduler for periodic tasks
  # --------------------------------------------------------------------------
  beat:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: curatore-beat
    command: celery -A app.celery_app beat -l info
    environment:
      - PYTHONUNBUFFERED=1
      # Database (PostgreSQL)
      - DATABASE_URL=${DATABASE_URL:-postgresql+asyncpg://curatore:curatore_dev_password@postgres:5432/curatore}
      - DB_POOL_SIZE=${DB_POOL_SIZE:-20}
      - DB_MAX_OVERFLOW=${DB_MAX_OVERFLOW:-40}
      - DB_POOL_RECYCLE=${DB_POOL_RECYCLE:-3600}
      - CELERY_BROKER_URL=${CELERY_BROKER_URL:-redis://redis:6379/0}
      - CELERY_RESULT_BACKEND=${CELERY_RESULT_BACKEND:-redis://redis:6379/1}
      - CELERY_DEFAULT_QUEUE=${CELERY_DEFAULT_QUEUE:-processing}
      # Enable scheduled task checking
      - SCHEDULED_TASK_CHECK_ENABLED=${SCHEDULED_TASK_CHECK_ENABLED:-true}
      - SCHEDULED_TASK_CHECK_INTERVAL=${SCHEDULED_TASK_CHECK_INTERVAL:-60}
      # Extraction service (match worker)
      - EXTRACTION_SERVICE_URL=http://extraction:8010
      - EXTRACTION_SERVICE_TIMEOUT=${EXTRACTION_SERVICE_TIMEOUT:-180}
      # Object Storage (match worker)
      - USE_OBJECT_STORAGE=${USE_OBJECT_STORAGE:-true}
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY:-${MINIO_ROOT_USER:-admin}}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY:-${MINIO_ROOT_PASSWORD:-changeme}}
      - MINIO_SECURE=${MINIO_SECURE:-false}
      - MINIO_BUCKET_UPLOADS=${MINIO_BUCKET_UPLOADS:-curatore-uploads}
      - MINIO_BUCKET_PROCESSED=${MINIO_BUCKET_PROCESSED:-curatore-processed}
      - MINIO_BUCKET_TEMP=${MINIO_BUCKET_TEMP:-curatore-temp}
    volumes:
      - ./backend/app:/app/app
      - ./backend/data:/app/data
      - ./config.yml:/app/config.yml:ro
    depends_on:
      - redis
      - backend

  # --------------------------------------------------------------------------
  # REDIS: Broker/result backend
  # --------------------------------------------------------------------------
  redis:
    image: redis:7-alpine
    container_name: curatore-redis
    ports:
      - "6379:6379"
    command: ["redis-server", "--save", "", "--appendonly", "no"]

  # --------------------------------------------------------------------------
  # FRONTEND: Next.js app
  # --------------------------------------------------------------------------
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: curatore-frontend
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL:-http://localhost:8000}
      - NEXT_PUBLIC_EXTRACTION_URL=${NEXT_PUBLIC_EXTRACTION_URL:-http://localhost:8010}
      - NEXT_PUBLIC_DOCLING_URL=${NEXT_PUBLIC_DOCLING_URL:-http://localhost:5151}
      - NEXT_PUBLIC_REDIS_URL=${NEXT_PUBLIC_REDIS_URL:-http://localhost:6379}
    command: npm run dev
    ports:
      - "3000:3000"
    depends_on:
      - backend

  # --------------------------------------------------------------------------
  # EXTRACTION: Document extraction microservice (optional but recommended)
  # --------------------------------------------------------------------------
  extraction:
    build:
      context: ./extraction-service
      dockerfile: Dockerfile
    container_name: curatore-extraction
    volumes:
      # Hot-reload extraction service code
      - ./extraction-service/app:/app/app
    ports:
      - "8010:8010"
    environment:
      - CORS_ORIGINS=["http://localhost:3000","http://127.0.0.1:3000"]
      - OCR_LANG=${OCR_LANG:-eng}
      - OCR_PSM=${OCR_PSM:-3}
    # Run with reload for live dev edits
    command: uvicorn app.main:app --host 0.0.0.0 --port 8010 --reload


  # --------------------------------------------------------------------------
  # PLAYWRIGHT: Browser rendering service for JavaScript-heavy web scraping
  # --------------------------------------------------------------------------
  playwright:
    build:
      context: ./playwright-service
      dockerfile: Dockerfile
    container_name: curatore-playwright
    restart: unless-stopped
    ports:
      - "8011:8011"
    environment:
      - PYTHONUNBUFFERED=1
      - DEBUG=${DEBUG:-false}
      - BROWSER_POOL_SIZE=${PLAYWRIGHT_BROWSER_POOL_SIZE:-3}
      - BROWSER_HEADLESS=true
      - DEFAULT_TIMEOUT_MS=${PLAYWRIGHT_DEFAULT_TIMEOUT_MS:-60000}
      - DEFAULT_WAIT_TIMEOUT_MS=${PLAYWRIGHT_DEFAULT_WAIT_TIMEOUT_MS:-5000}
    volumes:
      # Hot-reload playwright service code
      - ./playwright-service/app:/app/app
    deploy:
      resources:
        limits:
          memory: 4G
        reservations:
          memory: 2G
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8011/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    # Run with reload for live dev edits
    command: uvicorn app.main:app --host 0.0.0.0 --port 8011 --reload

  # --------------------------------------------------------------------------
  # POSTGRES: PostgreSQL database with pgvector extension
  # (enable with ENABLE_POSTGRES_SERVICE=true)
  # --------------------------------------------------------------------------
  postgres:
    profiles:
      - postgres
    image: pgvector/pgvector:pg16
    container_name: curatore-postgres
    restart: unless-stopped
    ports:
      - "5432:5432"
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-curatore}
      POSTGRES_USER: ${POSTGRES_USER:-curatore}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-curatore_dev_password}
      # Performance tuning for development
      POSTGRES_INITDB_ARGS: "--encoding=UTF-8"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-curatore} -d ${POSTGRES_DB:-curatore}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # --------------------------------------------------------------------------
  # DOCLING: As a second Extraction Engine option
  # --------------------------------------------------------------------------
  docling:
    profiles:
      - docling
    image: ghcr.io/docling-project/docling-serve-cpu:latest
    container_name: curatore-docling
    restart: unless-stopped
    ports:
      - "5151:5001"
    environment:
      DOCLING_SERVE_ENABLE_UI: "1"
      DOCLING_SERVE_ENABLE_REMOTE_SERVICES: "1"
      DOCLING_SERVE_ARTIFACTS_PATH: ""
      DOCLING_SERVE_MAX_SYNC_WAIT: ${DOCLING_SERVE_MAX_SYNC_WAIT:-300}
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # --------------------------------------------------------------------------
  # MINIO: S3-compatible object storage (REQUIRED)
  # --------------------------------------------------------------------------
  minio:
    image: quay.io/minio/minio:latest
    container_name: curatore-minio
    ports:
      - "9000:9000"    # S3 API
      - "9001:9001"    # Console
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-admin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-changeme}
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  postgres_data:
  minio_data:
